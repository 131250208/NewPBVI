POMDPSolver——程序的入口
加载POMDP模型，调用的方法是POMDP.load(sFileName)，对应的文件Models/Wumpus7_0.POMDP
三步:1.POMDPLoader.load(sFileName)
     2.initStoredRewards();  可选
     3.initBeliefStateFactory();
     
     2.3都在load里了
	 
读取文件Models/Wumpus7_0.POMDP
1.读取所有的states,actions,observations,discount等
2.初始化各种重要的函数
  状态转移函数T(s,a,s')
  回报值函数R(s,a,s')
  观察函数O(s,a,z)

加载之后，实例化了POMDP对象
POMDP对象中以下参数被初始化
m_dGamma  折扣因子

m_cStates 状态总数
m_mStates 保存状态的一个map，key为状态名称，value为其索引
m_vStateNames  状态名称向量，一个Vector

m_cActions 动作总数
m_mActionIndexes  保存动作的一个map，key为动作名称，value为其索引
m_vActionNames  动作名称向量，一个Vector

m_cObservations 观察总数
m_mObservations 保存观察的一个map，key为观察名称，value为其索引

m_fTransition 转移函数T(s,a,s'),初始化为一个SparseTabularFunction实例。      
SparseTabularFunction中初始化m_mTripleParameterValues为一个二维数组，每个数组元素是一个HashMap,实际上就是一个三维数组。

m_fObservation 观察函数O(a,s,o),初始化为一个SparseTabularFunction实例。

m_rtReward 回报值类型
m_fReward 回报函数R(s,a,s'),初始化为SparseTabularFunction或者TabularFunction
m_adMinActionRewards 动作的最小回报值，一个一维数组，索引为动作
m_dMinReward 最小的回报值

m_fStartState 开始状态，初始化为一个SparseTabularFunction或者TabularFunction

m_vTerminalStates 一个向量Vector，保存结束状态

m_vObservationStates 一个向量Vector，保存观察状态

m_bsFactory  信念工厂

m_vfMDP  初始化为MDP值函数

--------------------------------------------------------------------------------------------------------------------------------

值迭代ValueIteration
ValueIteration viAlgorithm = new PointBasedValueIteration(pomdp);
viAlgorithm.valueIteration(int cIteration, double dEpsilon, double dTargetValue, int maxRunningTime, int numEvaluations);
其中cIterations为最大迭代次数；dEpsilon为结束条件，即值迭代前后的value之差的最大值；
通过值迭代，求出一个策略及相应的值函数

这里迭代400次，外部迭代终止条件为迭代达到了400次或者每次迭代扩充后的信念点集不再发生变化

1.初始化信念点集
BeliefStateVector vBeliefPoints = new BeliefStateVector();
将一个初始信念状态添加到vBeliefPoints中
vBeliefPoints.add(initialBeliefState);
//这里的initialBeliefState是通过BeliefStateFactory.getInitialBeliefState()方法得到的
//获取初始的信念点主要就是两件是：(1)新建一个信念状态  (2)设置每个可能状态上的概率————这通过POMDP类可以获取


//下面开始PBVI算法
2.扩充信念点集
expandPBVI(vBeliefPoints);
(1)先记扩充后的信念点集为初始信念点集
  vExpanded = vBeliefPoints
(2)然后开始扩充，在信念点集大小小于100时，每次都是扩充一倍大小;大于100时，每次扩充100个
    while(vExpanded.size()<expandSize)
    {
        (3)从vExpanded中随机取个b，计算它的最远后继！！和标准PBVI中expand不同
        bsCurrent = vExpanded.elementAt(m_rndGenerator.nextInt(vExpanded.size()));

        (4)计算该信念点的最远后继
        computRandomFarthestSuccessor(vBeliefPoints, bsCurrent)
		                      |
            (4.1)根据当前信念状态bs，随机取一个动作iAction，然后遍历所有观察
			iAction = m_rndGenerator.nextInt(m_pPOMDP.getActionCount());
            for(iObservation = 0; iObservation<cObservations; iObservation++)
            {
                (4.2)根据iAcion和iObservation得到后继信念点
                bsNext = bs.nextBeliefState(iAction, iObservation);
				
                (4.3)计算每个后继信念点bsNext与信念点集vBeliefPoints的距离
                dDist = distance(vBeliefPoints, bsNext);
			                   |
			        (4.3.1)遍历vBeliefStates,求出vBeliefState中每个信念点和bsNext之间的L1距离					
				    Iterator<BeliefState> it = vBeliefStates.iterator();
			        while(it.hasNext())
				    {
				        bsCurrent = (BeliefState)it.next();
					    dDist = L1Distance.distance(bs, bsCurrent);
				    }
                    (4.3.2)比较得出最小的L1距离dMinDist,即为信念点集vBeliefPoints与信念点bsNext之间的距离
				//dDist = dMinDist
				
            }
			(4.4)比较得出最大的dMaxDist = max(dDist)
				这里需要注意：上面第8步中的距离是vBeliefStates这个信念点集中每个信念点与bsNext之间的距离的最小值
				而这里的最大距离是指：每个可能的后继bsNext与vBeliefStates之间都有一个距离,这个距离的最大值即为dMaxDist
		(5)添加与最大距离dMaxDist相对应的bsNext,将这个bsNext加入到扩充的信念点集中
        vExpanded.add(bsCurrent, bsNext);//这里bsNext是基于bsCurrent得到的————随机取一个action,遍历所有的observation
    }
(6)扩充结束


3.更新值函数
improveValueFunction(vBeliefPoints);
两个重要的步骤：
第一步：对于每一个信念状态，计算信念状态执行每个动作后的回报值。从而得到最佳动作和最大回报值。
第二步：新建新的值函数（alpha向量），并设置其在每个状态下的函数值。

(1)迭代所有的信念点
m_itCurrentIterationPoints = vBeliefPoints.getTreeDownUpIterator();
while(m_itCurrentIterationPoints.hasNext())
{
    (2)获取当前的信念点
    bsCurrent = m_itCurrentIterationPoints.next();
    (3)根据当前信念点从值函数集合中获得对应的最大的alpha向量
    avCurrentMax = m_vValueFunction.getMaxAlpha(bsCurrent);
	                     |
		(3.1)遍历值函数集合中的所有的alpha向量————m_vAlphaVectors
        Iterator<AlphaVector> itBackward = m_vAlphaVecotors.backwardIterator();
		while(itBackward.hasNext() && !bDone)
		{
		    (3.2)求每个alpha向量与bsCurrent的内积
			AlphaVector avCurrent = itBackward.next();
			dValue = avCurrent.dotProduct(bs);//求内积
			if(dValue>dMaxValue)
			...
		}
		(3.3)比较得到内积最大的alpha向量并返回
		return avMaxAlpha;
		
    (4)根据当前信念点进行backup操作，并得到backup操作后的alpha向量
    avBackup = backup(bsCurrent);
        这里很重要，流程如下
        ValueIteration.backup(BeliefState bs);
                       |
            ValueIteration.backup(BeliefState bs, LinearValueFunctionApproximation vValueFunction)
                           |
                avResult = backupTauBased(bs, vValueFunction);
                               |
                    在backupTauBased(bs, vValueFunction)中有如下流程：
					(4.1)新建两个Vector，分别用来保存最优的动作和最优动作对应的α向量
				    Vector<AlphaVector[]> vWinners = new Vector<AlphaVector[]>();
				    Vector<Integer> vWinnersActions = new Vector<Integer>();
				                   
                    (4.2)迭代所有的action，计算出每个action下的回报值并比较得出最大的回报值
                    for(int iAction=0;iAction<m_cActions;iAction++)
                    {                          
                        aNext = new AlphaVector[m_cObservations];
						//aNext存放由b,a,V,找到每个o迭代一步后的最大alpha向量
						//返回值为F10，类似b、a固定时，迭代一步后计算得到的最大value，在每个o迭代一步后对应的最大α条件下
						(4.3)计算每个动作下的回报值
						dValue = findMaxAlphas(iAction, bs, vValueFunction, aNext);
						                |
							(4.3.1)遍历所有的观察
							for(iObservation = 0; iObservation<m_cObservations;iObservation++)
							{
							    (4.3.2)求出信念状态bs执行iAction观察到iObservation的概率———— sum[b(s)*sum[T(s,a,s')*O(a,s',o)]]
								dProb = bs.probabilityOGivenA(iAction, iObservation);
								dSumProbs += dProb;
								
								(4.3.3)信念状态bs根据iAction和iObservation生成后继信念状态————两步：1.新建一个信念状态 2.setValue(s',sum[b(s)*T(s,a,s')*O(a,s',o)])
								bsSuccessor = bs.nextBeliefState(iAction, iObservation);
								(4.3.4)根据后继信念状态bsSuccessor从值函数集合中找到最大的alpha向量————遍历所有的alpha向量，求得与bsSuccessor内积最大的那个
								avAlpha = vValueFunction.getMaxAlpha(bsSuccessor);
								(4.3.5)最大的alpha向量和bsSuccessor的内积
								dValue = avAlpha.dotProduct(bsSuccessor);
								
								dSumValues += dValue*dProb;//迭代一步后的回报总和
							}							
							dSumValues /= dSumProbs;
							dSumValues *= m_pPOMDP.getDiscountFactor();//乘以折扣因子
							dSumValues += m_pPOMDP.immediateReward(bs, iAction);//加上直接回报
                            //见论文26页公式2-21   Vn(b) = w(b,a) + r*sum[φ(b,a,o)*Vn-1(b')]							
							return dSumValues;//返回的就是每个动作下的总的回报值
												                          
                        (4.4)获取最大的回报值dValue，将相应的alpha向量avCurrent和最佳动作iMaxAction添加到vWinners和vWinnersActions中。
					    if(dValue>dMaxValue)
					    {
					        dMaxValue = dValue;
						    vWinners.clear();
						    vWinnersActions.clear();
					    }
					    if(dValue == dMaxValue)
					    {
					        aBest = aNext;
							iMaxAction = iAction;
						    vWinners.add(aBest);
						    vWinnersActions.add(iMaxAction);
					    }
                    }					
                    (4.5)从vWinners中随机取一个alpha向量数组和动作，这必然是最佳动作
				    int idx = m_rndGenerator.nextInt(vWinners.size());
					aBest = vWinners.elementAt(idx);//这里aBest是迭代一步后每个观察下的最优alpha向量数组
				    iMaxAction = vWinnersActions.elementAt(idx);//最佳动作
					
					第一大步到此结束。已得到最佳动作，最大回报值，每个观察值下的最优值函数。
					下面是第二大步。
				    
					(4.6)计算出更新后的α————上面所有的α向量均是值函数集合中的，这里的α向量是新计算的
					avMax = G(iMaxAction, vValueFunction, aBest);
					                   |
					    在方法G(iMaxAction, vValueFunction, aBest)中有如下流程：
						(4.6.1)遍历所有的观察
						for(iObservation = 0;iObservation<m_cObservations;iObservation++)
						{
						    (4.6.2)得到每一个观察值对应的alpha向量
							avAlpha = aBest[iObservation];
							(4.6.3)计算出一个新的alpha向量，并设置每个状态下的函数值————通过G(a,o)操作
							avG = avAlpha.G(iAction, iObservation);
							               |								
								computeG(iAction, iObservation)
								方法computeG的主要流程如下：
								(4.6.3.1)新建一个alpha向量并设置iAction
								AlphaVector avResult = newAlphaVector();
								avResult.setAction(iAction);
								(4.6.3.2)遍历所有的初始状态
								for( iStartState = 0 ; iStartState < m_cStates ; iStartState++ )
								{
								    //根据iStartState和iAction得到所有的转移函数T(s,a,s')
									itNonZeroEntries = POMDP.getNonZeroTransitions(iStartState, iAction);
									while(itNonZeroEntries.hasNext())
									{
									    eValue = itNonZeroEntries.next();
										iEndState = eValue.getKey();
										dValue = valueAt(iEndState);//alpha(s')————alpha向量在该状态的值
										dTr = eValue.getValue();//T(s,a,s')转移到s'的概率
										dObservation = pomdp.O(iAction, iEndState, iObservation);
										
										dSum += dObservation*dTr*dValue;   // sum[O(a,s',o)*T(s,a,s')*alpha(s')]
									}									
									(4.6.3.3)设置每个状态下的函数值————这个值是根据iAction和iObservation决定的
									avResult.setValue(iStartState, dSum);//状态iStartState，执行action，观察到iObservation后的函数值
								}
								return avResult;
								
							    //avG即avResult
							(4.6.4)累加每个观察下的avG,存入avSum————将观察到每个o后的函数值累加
							avSum.accumlate(avG);
						}
						(4.6.5)增加立即回报
						avResult = avSum.addReward(iAction);  //R(s,a) + r*sum[O(a,o)*sum[T(s,a,s')*O(a,s',o)*alpha(s')]]
						return avResult;
						//avMax即avResult
				    avMax.setWitness(bs);//设置目击者
				    return avMax;

    (5)计算backup前后，该信念点和alpha向量的内积之差
    dBackupValue = avBackup.dotProduct(bsCurrent);
    dValue = avCurrentMax.dotProduct(bsCurrent);
    dDelta = dBackupValue - dValue;
    
    (6)如果dDelta>=0，则将avBackup添加到值函数集合m_vValueFunction中
    if(dDelta>=0)
	{
	    m_vValueFunction.addPrunePointwiseDominated(avBackup);
	}
}
dDelta = improveValueFunction()方法返回的是backup前后一个信念点与alpha向量内积的差的最大值。
dDelta<dEpsilon且值函数不变了则内部迭代结束
否则检查平均折扣回报是否收敛，如果收敛，就认为内部迭代可以停止了,检查收敛的方法为checkADRConvergence();















